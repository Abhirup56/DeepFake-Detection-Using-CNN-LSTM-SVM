{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqbNFT6cPbPx"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-KHk4SxlLkZ"
      },
      "outputs": [],
      "source": [
        "# !ls drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9YnIxixGnaOk",
        "outputId": "5570f273-c99f-4779-846e-aa41eef0303f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HU9boSanYJD"
      },
      "outputs": [],
      "source": [
        "# # Libraries\n",
        "# import os, random\n",
        "# import cv2\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # frames path\n",
        "# raw_root = \"/content/drive/MyDrive/frames\"\n",
        "# REAL_DIR = \"/content/drive/MyDrive/frames/reals 2.o\"\n",
        "# FAKE_DIR = \"/content/drive/MyDrive/frames/fakes\""
      ],
      "metadata": {
        "id": "NtrjBAFDP3ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K-qPdzsGz_Y"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# print(\"Checking REAL_DIR:\", REAL_DIR, \" → files:\", len(os.listdir(REAL_DIR)) if os.path.exists(REAL_DIR) else \"MISSING\")\n",
        "# print(\"Checking FAKE_DIR:\", FAKE_DIR, \" → files:\", len(os.listdir(FAKE_DIR)) if os.path.exists(FAKE_DIR) else \"MISSING\")\n",
        "\n",
        "# # List first few files\n",
        "# if os.path.exists(REAL_DIR):\n",
        "#     print(\"Sample real frames:\", os.listdir(REAL_DIR)[:10])\n",
        "# if os.path.exists(FAKE_DIR):\n",
        "#     print(\"Sample fake frames:\", os.listdir(FAKE_DIR)[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HOuWbNaeVr6n"
      },
      "outputs": [],
      "source": [
        "# def show_samples(label, num=2):\n",
        "#     folder = os.path.join(raw_root, label)\n",
        "#     files  = random.sample(os.listdir(folder), min(len(os.listdir(folder)), num))\n",
        "#     plt.figure(figsize=(6,6))\n",
        "#     for i, f in enumerate(files):\n",
        "#         img = cv2.cvtColor(\n",
        "#             cv2.imread(os.path.join(folder, f)),\n",
        "#             cv2.COLOR_BGR2RGB\n",
        "#         )\n",
        "#         plt.subplot(2,2,i+1)\n",
        "#         plt.imshow(img)\n",
        "#         plt.title(f\"{label}: {f}\")\n",
        "#         plt.axis('off')\n",
        "#     plt.show()\n",
        "\n",
        "# print(\"Sample FAKE faces:\")\n",
        "# show_samples('fakes')\n",
        "# print(\"\\nSample REAL faces:\")\n",
        "# show_samples('reals 2.o')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Drive (run in Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2) Paths (your uploaded location)\n",
        "RAW_ROOT = \"/content/drive/MyDrive/frames\"\n",
        "REAL_DIR = \"/content/drive/MyDrive/frames/reals 2.o\"   # your folder name (note the space + dot)\n",
        "FAKE_DIR = \"/content/drive/MyDrive/frames/fakes\"\n",
        "\n",
        "# Quick checks\n",
        "import os\n",
        "print(\"RAW_ROOT exists:\", os.path.isdir(RAW_ROOT))\n",
        "print(\"REAL_DIR exists:\", os.path.isdir(REAL_DIR), \" ->\", len(os.listdir(REAL_DIR)) if os.path.isdir(REAL_DIR) else \"MISSING\")\n",
        "print(\"FAKE_DIR exists:\", os.path.isdir(FAKE_DIR), \" ->\", len(os.listdir(FAKE_DIR)) if os.path.isdir(FAKE_DIR) else \"MISSING\")\n",
        "\n",
        "# show few filenames (to confirm naming convention)\n",
        "if os.path.isdir(REAL_DIR):\n",
        "    print(\"Few real filenames:\", os.listdir(REAL_DIR)[:10])\n",
        "if os.path.isdir(FAKE_DIR):\n",
        "    print(\"Few fake filenames:\", os.listdir(FAKE_DIR)[:10])\n"
      ],
      "metadata": {
        "id": "bh5htvEYCiv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Group frames by video base name\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "SEQ_LEN = 16   # frames per sequence (you can change later)\n",
        "def collect_video_sequences(folder, seq_len=SEQ_LEN):\n",
        "    files = sorted([f for f in os.listdir(folder) if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
        "    groups = defaultdict(list)\n",
        "    # attempt common filename patterns:\n",
        "    # expects names like: vid001_f001.jpg  or anything that has \"_f<number>\"\n",
        "    pat = re.compile(r\"^(.*)_f\\d+\", re.IGNORECASE)\n",
        "    for fn in files:\n",
        "        m = pat.match(fn)\n",
        "        if m:\n",
        "            base = m.group(1)\n",
        "        else:\n",
        "            # fallback: everything before last underscore (if present)\n",
        "            if \"_\" in fn:\n",
        "                base = \"_\".join(fn.split(\"_\")[:-1])\n",
        "            else:\n",
        "                base = os.path.splitext(fn)[0]  # treat full name as base\n",
        "        groups[base].append(os.path.join(folder, fn))\n",
        "    # produce only sequences with >=1 frames, trim to seq_len (we will pad later if needed)\n",
        "    sequences = []\n",
        "    for base, flist in groups.items():\n",
        "        flist_sorted = sorted(flist, key=lambda p: int(re.search(r\"_f(\\d+)\", os.path.basename(p) or \"\")[1]) if re.search(r\"_f(\\d+)\", os.path.basename(p) or \"\") else os.path.basename(p))\n",
        "        if len(flist_sorted) >= 1:\n",
        "            sequences.append(flist_sorted)\n",
        "    return sequences\n",
        "\n",
        "print(\"Collecting sequences (this might take a moment)...\")\n",
        "real_seq = collect_video_sequences(REAL_DIR, SEQ_LEN)\n",
        "fake_seq = collect_video_sequences(FAKE_DIR, SEQ_LEN)\n",
        "print(\"Video sequences found -> real:\", len(real_seq), \" fake:\", len(fake_seq))\n"
      ],
      "metadata": {
        "id": "gfVpiJJfCnHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Choose how many videos per class to use (change SAMPLE_PER_CLASS as needed)\n",
        "MIN_AVAILABLE = min(len(real_seq), len(fake_seq))\n",
        "SAMPLE_PER_CLASS = min(5000, MIN_AVAILABLE)   # default: up to 5000 per class\n",
        "print(\"Min available per class:\", MIN_AVAILABLE)\n",
        "print(\"Using sample per class:\", SAMPLE_PER_CLASS)\n",
        "\n",
        "# 5) Randomly sample balanced sequences\n",
        "import random\n",
        "RNG = 42\n",
        "random.seed(RNG)\n",
        "real_sample = random.sample(real_seq, SAMPLE_PER_CLASS)\n",
        "fake_sample = random.sample(fake_seq, SAMPLE_PER_CLASS)\n",
        "\n",
        "# 6) Create a local copy for faster training\n",
        "LOCAL_ROOT = \"/content/local_dataset\"   # Colab VM (fast)\n",
        "import shutil, os\n",
        "def prepare_local_copy(sequences, out_folder):\n",
        "    os.makedirs(out_folder, exist_ok=True)\n",
        "    for i, seq in enumerate(tqdm(sequences, desc=f\"Copying to {out_folder}\")):\n",
        "        # create a folder per video: video_000001, video_000002, ...\n",
        "        vd = os.path.join(out_folder, f\"video_{i:06d}\")\n",
        "        os.makedirs(vd, exist_ok=True)\n",
        "        for j, src in enumerate(seq):\n",
        "            dst = os.path.join(vd, f\"f{j:04d}.jpg\")\n",
        "            # copy (or use hardlink if on same fs, but here copy)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "out_real = os.path.join(LOCAL_ROOT, \"real\")\n",
        "out_fake = os.path.join(LOCAL_ROOT, \"fake\")\n",
        "# Copy\n",
        "prepare_local_copy(real_sample, out_real)\n",
        "prepare_local_copy(fake_sample, out_fake)\n",
        "print(\"Local copy prepared:\", LOCAL_ROOT)\n"
      ],
      "metadata": {
        "id": "NRbnxqd4CoDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Keras Sequence to yield batches of sequences\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import math\n",
        "\n",
        "IMG_SIZE = 128\n",
        "SEQ_LEN = 16\n",
        "BATCH_SIZE = 8  # adjust to fit GPU memory (lower if OOM)\n",
        "\n",
        "def preprocess_img_cv2(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        # fallback black image if read fails\n",
        "        img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "class VideoSequence(Sequence):\n",
        "    def __init__(self, video_folders, labels, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, shuffle=True, augment=False):\n",
        "        self.video_folders = video_folders\n",
        "        self.labels = np.array(labels)\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indexes = np.arange(len(self.video_folders))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.video_folders) / float(self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_idx = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for i in batch_idx:\n",
        "            folder = self.video_folders[i]\n",
        "            imgs = sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(('.jpg','.png'))])\n",
        "            # choose central window if more than seq_len; otherwise pad by repeating last frame\n",
        "            n = len(imgs)\n",
        "            if n >= self.seq_len:\n",
        "                start = max(0, (n - self.seq_len)//2)\n",
        "                chosen = imgs[start:start+self.seq_len]\n",
        "            else:\n",
        "                chosen = imgs + [imgs[-1]]*(self.seq_len - n)\n",
        "            frames = np.stack([preprocess_img_cv2(p) for p in chosen], axis=0)  # (seq_len, H, W, 3)\n",
        "            # optionally augment per-frame (simple horizontal flip)\n",
        "            if self.augment and np.random.rand() < 0.5:\n",
        "                frames = frames[:, :, ::-1, :]  # flip horizontally\n",
        "            batch_x.append(frames)\n",
        "            batch_y.append(self.labels[i])\n",
        "        batch_x = np.array(batch_x, dtype=np.float32)  # (batch, seq_len, H, W, 3)\n",
        "        batch_y = np.array(batch_y, dtype=np.float32)\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n"
      ],
      "metadata": {
        "id": "LETWqxVHCutD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Build lists of local video folders (we copied them earlier)\n",
        "import glob\n",
        "real_folders = sorted(glob.glob(os.path.join(out_real, \"video_*\")))\n",
        "fake_folders = sorted(glob.glob(os.path.join(out_fake, \"video_*\")))\n",
        "\n",
        "print(\"Local video folders -> real:\", len(real_folders), \" fake:\", len(fake_folders))\n",
        "\n",
        "# balanced lists\n",
        "min_count = min(len(real_folders), len(fake_folders))\n",
        "real_folders = random.sample(real_folders, min_count)\n",
        "fake_folders = random.sample(fake_folders, min_count)\n",
        "\n",
        "all_folders = real_folders + fake_folders\n",
        "labels = [0]*len(real_folders) + [1]*len(fake_folders)\n",
        "\n",
        "# shuffle combined\n",
        "pairs = list(zip(all_folders, labels))\n",
        "random.shuffle(pairs)\n",
        "all_folders, labels = zip(*pairs)\n",
        "all_folders = list(all_folders)\n",
        "labels = list(labels)\n",
        "\n",
        "# splits (70/15/15)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_f, rest_f, train_y, rest_y = train_test_split(all_folders, labels, test_size=0.30, random_state=42, stratify=labels)\n",
        "val_f, test_f, val_y, test_y = train_test_split(rest_f, rest_y, test_size=0.50, random_state=42, stratify=rest_y)\n",
        "\n",
        "print(\"Splits -> train:\", len(train_f), \" val:\", len(val_f), \" test:\", len(test_f))\n",
        "\n",
        "# create generators\n",
        "train_gen = VideoSequence(train_f, train_y, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, shuffle=True, augment=True)\n",
        "val_gen   = VideoSequence(val_f, val_y, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, shuffle=False, augment=False)\n",
        "test_gen  = VideoSequence(test_f, test_y, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, shuffle=False, augment=False)\n"
      ],
      "metadata": {
        "id": "yPMURLWJCxpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Build model (TimeDistributed frame encoder -> BiLSTM -> Dense)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "IMG_SIZE = 128\n",
        "FEATURE_DIM = 512  # projection after frame encoder\n",
        "# Frame encoder (EfficientNetB0, frozen initially)\n",
        "base_cnn = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "base_cnn.trainable = False\n",
        "frame_input = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = tf.keras.applications.efficientnet.preprocess_input(frame_input*255.0)  # our generator returns [0,1]\n",
        "x = base_cnn(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "frame_encoder = models.Model(frame_input, x, name=\"frame_encoder\")\n",
        "\n",
        "# Sequence model\n",
        "video_input = layers.Input(shape=(SEQ_LEN, IMG_SIZE, IMG_SIZE, 3), name=\"video_input\")\n",
        "# apply encoder to each frame\n",
        "encoded = layers.TimeDistributed(frame_encoder)(video_input)   # (batch, seq_len, feat_dim)\n",
        "proj = layers.TimeDistributed(layers.Dense(FEATURE_DIM, activation='relu'))(encoded)\n",
        "lstm = layers.Bidirectional(layers.LSTM(256, return_sequences=False, dropout=0.2))(proj)\n",
        "x = layers.Dense(128, activation='relu')(lstm)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "out = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = models.Model(video_input, out)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "nYanbf4MC1aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Compute class weights\n",
        "import numpy as np\n",
        "from sklearn.utils import class_weight\n",
        "train_labels_array = np.array(train_y)\n",
        "cw = class_weight.compute_class_weight(\"balanced\", classes=np.unique(train_labels_array), y=train_labels_array)\n",
        "class_weights = {i: w for i, w in enumerate(cw)}\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# callbacks\n",
        "checkpoint_path = \"/content/cnn_lstm_best.h5\"\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_auc', mode='max'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=6, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Train (set epochs as needed)\n",
        "EPOCHS = 12\n",
        "history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, callbacks=callbacks, class_weight=class_weights)\n",
        "model.save(\"/content/drive/MyDrive/deepfake_model_v1.h5\")\n"
      ],
      "metadata": {
        "id": "nDzrFsizC2Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Evaluate\n",
        "model.load_weights(checkpoint_path)\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_prob = []\n",
        "for Xb, yb in test_gen:\n",
        "    probs = model.predict(Xb)\n",
        "    preds = (probs.ravel() > 0.5).astype(int)\n",
        "    y_true.extend(yb.tolist())\n",
        "    y_prob.extend(probs.ravel().tolist())\n",
        "    y_pred.extend(preds.tolist())\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_true, y_prob))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# Save final model\n",
        "model.save(\"/content/cnn_lstm_final.h5\")\n",
        "print(\"Saved model to /content/cnn_lstm_final.h5\")\n"
      ],
      "metadata": {
        "id": "wdwoR1dCC7FA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}